#!/usr/bin/env python3
"""
Generation 2 Robust Demo - MAKE IT ROBUST
==========================================

Demonstrates enhanced functionality with comprehensive error handling,
validation, logging, monitoring, and security measures.
"""

import asyncio
import json
import logging
import time
from datetime import datetime, UTC
from typing import Dict, List, Any

from src.agent_skeptic_bench import (
    SkepticBenchmark,
    MockSkepticAgent,
    AgentConfig,
    AgentProvider,
    EvaluationMetrics,
    EvaluationResult
)
from src.agent_skeptic_bench.evaluation import SkepticismEvaluator
from src.agent_skeptic_bench.security import InputValidator, RateLimiter


class RobustEvaluationPipeline:
    """Enhanced evaluation pipeline with robust error handling and monitoring."""
    
    def __init__(self):
        # Configure logging
        self._setup_logging()
        
        # Initialize security components
        self.input_validator = InputValidator()
        self.rate_limiter = RateLimiter()
        
        # Configure rate limiting
        from src.agent_skeptic_bench.security.rate_limiting import RateLimitConfig, RateLimitStrategy, RateLimitScope
        config = RateLimitConfig(
            name="evaluation_limit",
            strategy=RateLimitStrategy.SLIDING_WINDOW,
            scope=RateLimitScope.GLOBAL,
            limit=100,
            window_seconds=60
        )
        self.rate_limiter.add_config(config)
        
        # Initialize monitoring
        self.metrics = {
            "evaluations_total": 0,
            "evaluations_successful": 0,
            "evaluations_failed": 0,
            "total_response_time": 0.0,
            "errors": []
        }
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("Robust evaluation pipeline initialized")
    
    def _setup_logging(self):
        """Configure comprehensive logging."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('generation_2_robust.log', mode='w')
            ]
        )
    
    async def run_robust_evaluation(self) -> Dict[str, Any]:
        """Run evaluation with comprehensive error handling and validation."""
        self.logger.info("Starting Generation 2 robust evaluation")
        start_time = time.time()
        
        try:
            # Step 1: Initialize with validation
            await self._initialize_with_validation()
            
            # Step 2: Create and validate agent
            agent = await self._create_validated_agent()
            
            # Step 3: Load and validate scenarios
            scenarios = await self._load_validated_scenarios()
            
            # Step 4: Run evaluations with error handling
            results = await self._run_evaluations_with_monitoring(agent, scenarios)
            
            # Step 5: Generate comprehensive report
            report = await self._generate_robust_report(results, start_time)
            
            self.logger.info("Generation 2 evaluation completed successfully")
            return report
            
        except Exception as e:
            self.logger.error(f"Critical error in robust evaluation: {e}", exc_info=True)
            self.metrics["evaluations_failed"] += 1
            return await self._generate_error_report(e, start_time)
    
    async def _initialize_with_validation(self):
        """Initialize benchmark with comprehensive validation."""
        self.logger.info("Initializing benchmark with validation")
        
        try:
            self.benchmark = SkepticBenchmark()
            self.evaluator = SkepticismEvaluator()
            
            # Validate benchmark state
            if not hasattr(self.benchmark, 'get_scenarios'):
                raise ValueError("Benchmark missing required methods")
            
            self.logger.info("âœ… Benchmark initialization validated")
            
        except Exception as e:
            self.logger.error(f"Benchmark initialization failed: {e}")
            raise
    
    async def _create_validated_agent(self) -> MockSkepticAgent:
        """Create agent with input validation and security checks."""
        self.logger.info("Creating validated mock agent")
        
        try:
            # Validate agent configuration
            config_data = {
                "provider": "custom",
                "model_name": "robust-mock-skeptic-v2",
                "api_key": "validated-demo-key",
                "temperature": 0.5
            }
            
            # Apply input validation
            validated_config = self.input_validator.validate_agent_config(config_data)
            
            config = AgentConfig(
                provider=AgentProvider.CUSTOM,
                model_name=validated_config["model_name"],
                api_key=validated_config["api_key"],
                temperature=validated_config["temperature"]
            )
            
            agent = MockSkepticAgent(config)
            
            # Validate agent functionality
            if not hasattr(agent, 'evaluate_claim'):
                raise ValueError("Agent missing required evaluation method")
            
            self.logger.info(f"âœ… Agent created and validated: {agent.agent_id}")
            return agent
            
        except Exception as e:
            self.logger.error(f"Agent creation failed: {e}")
            raise
    
    async def _load_validated_scenarios(self) -> List:
        """Load scenarios with validation and sanitization."""
        self.logger.info("Loading validated scenarios")
        
        try:
            # Apply rate limiting
            rate_result = await self.rate_limiter.check_rate_limit("evaluation_limit", "global")
            if not rate_result.allowed:
                raise Exception("Rate limit exceeded")
            
            scenarios = self.benchmark.get_scenarios(limit=5)
            
            # Validate scenario data
            for scenario in scenarios:
                if not scenario.id or not scenario.name:
                    raise ValueError(f"Invalid scenario data: {scenario}")
                
                # Sanitize input data
                scenario.name = self.input_validator.sanitize_text(scenario.name)
                scenario.description = self.input_validator.sanitize_text(scenario.description)
            
            self.logger.info(f"âœ… {len(scenarios)} scenarios loaded and validated")
            return scenarios
            
        except Exception as e:
            self.logger.error(f"Scenario loading failed: {e}")
            raise
    
    async def _run_evaluations_with_monitoring(self, agent, scenarios) -> List[EvaluationResult]:
        """Run evaluations with comprehensive monitoring and error handling."""
        self.logger.info("Running evaluations with monitoring")
        results = []
        
        for i, scenario in enumerate(scenarios, 1):
            evaluation_start = time.time()
            
            try:
                self.logger.info(f"Evaluating scenario {i}/{len(scenarios)}: {scenario.name}")
                
                # Apply rate limiting per evaluation
                rate_result = await self.rate_limiter.check_rate_limit("evaluation_limit", "global")
                if not rate_result.allowed:
                    self.logger.warning(f"Rate limit reached, skipping scenario {i}")
                    continue
                
                # Validate scenario before evaluation
                if not scenario.adversary_claim.strip():
                    self.logger.warning(f"Scenario {i} has empty claim, skipping")
                    continue
                
                # Run evaluation with timeout
                response = await asyncio.wait_for(
                    agent.evaluate_claim(scenario),
                    timeout=30.0  # 30 second timeout
                )
                
                # Validate response
                if not response or not response.response_text:
                    raise ValueError("Invalid response from agent")
                
                # Evaluate metrics
                metrics = self.evaluator.evaluate_skepticism(scenario, response)
                
                # Create result
                result = EvaluationResult(
                    task_id=f"robust_eval_{i}",
                    scenario=scenario,
                    response=response,
                    metrics=metrics,
                    analysis={
                        "generation": "2_robust",
                        "validation_passed": True,
                        "monitoring_enabled": True,
                        "security_checked": True
                    }
                )
                
                results.append(result)
                
                # Update metrics
                eval_time = time.time() - evaluation_start
                self.metrics["evaluations_total"] += 1
                self.metrics["evaluations_successful"] += 1
                self.metrics["total_response_time"] += eval_time
                
                self.logger.info(f"âœ… Scenario {i} evaluated successfully in {eval_time:.3f}s")
                
            except asyncio.TimeoutError:
                error_msg = f"Evaluation timeout for scenario {i}"
                self.logger.error(error_msg)
                self.metrics["evaluations_failed"] += 1
                self.metrics["errors"].append(error_msg)
                
            except Exception as e:
                error_msg = f"Evaluation error for scenario {i}: {e}"
                self.logger.error(error_msg, exc_info=True)
                self.metrics["evaluations_failed"] += 1
                self.metrics["errors"].append(error_msg)
        
        self.logger.info(f"Evaluations completed: {len(results)} successful")
        return results
    
    async def _generate_robust_report(self, results: List[EvaluationResult], start_time: float) -> Dict[str, Any]:
        """Generate comprehensive report with monitoring data."""
        self.logger.info("Generating robust evaluation report")
        
        execution_time = time.time() - start_time
        
        if not results:
            self.logger.warning("No successful evaluations to report")
            return {
                "success": False,
                "error": "No successful evaluations",
                "execution_time": execution_time,
                "metrics": self.metrics
            }
        
        # Calculate performance metrics
        total_scenarios = len(results)
        passed_scenarios = sum(1 for r in results if r.passed_evaluation)
        avg_score = sum(r.metrics.overall_score for r in results) / total_scenarios
        avg_response_time = sum(r.response.response_time_ms for r in results) / total_scenarios
        
        # Calculate robust metrics
        success_rate = self.metrics["evaluations_successful"] / max(1, self.metrics["evaluations_total"])
        avg_eval_time = self.metrics["total_response_time"] / max(1, self.metrics["evaluations_successful"])
        
        report = {
            "generation": "2_robust",
            "timestamp": datetime.now(UTC).isoformat(),
            "execution_time": execution_time,
            "success": True,
            
            "evaluation_summary": {
                "scenarios_attempted": self.metrics["evaluations_total"],
                "scenarios_successful": self.metrics["evaluations_successful"],
                "scenarios_failed": self.metrics["evaluations_failed"],
                "success_rate": success_rate,
                "scenarios_passed_evaluation": passed_scenarios,
                "pass_rate": passed_scenarios / total_scenarios if total_scenarios > 0 else 0,
                "average_score": avg_score,
                "average_response_time_ms": avg_response_time,
                "average_evaluation_time": avg_eval_time
            },
            
            "robustness_metrics": {
                "error_count": len(self.metrics["errors"]),
                "timeout_handling": True,
                "input_validation": True,
                "rate_limiting": True,
                "security_checks": True,
                "logging_enabled": True,
                "monitoring_enabled": True
            },
            
            "detailed_metrics": {
                "skepticism_calibration": sum(r.metrics.skepticism_calibration for r in results) / total_scenarios,
                "evidence_standard_score": sum(r.metrics.evidence_standard_score for r in results) / total_scenarios,
                "red_flag_detection": sum(r.metrics.red_flag_detection for r in results) / total_scenarios,
                "reasoning_quality": sum(r.metrics.reasoning_quality for r in results) / total_scenarios
            },
            
            "errors": self.metrics["errors"],
            
            "results": [
                {
                    "scenario_id": r.scenario.id,
                    "scenario_name": r.scenario.name,
                    "category": r.scenario.category.value,
                    "passed": r.passed_evaluation,
                    "overall_score": r.metrics.overall_score,
                    "response_time_ms": r.response.response_time_ms,
                    "validation_status": "passed"
                }
                for r in results
            ]
        }
        
        self.logger.info(f"Report generated: {total_scenarios} scenarios, {success_rate:.1%} success rate")
        return report
    
    async def _generate_error_report(self, error: Exception, start_time: float) -> Dict[str, Any]:
        """Generate error report when critical failure occurs."""
        execution_time = time.time() - start_time
        
        return {
            "generation": "2_robust",
            "timestamp": datetime.now(UTC).isoformat(),
            "execution_time": execution_time,
            "success": False,
            "error": str(error),
            "metrics": self.metrics,
            "robustness_features": {
                "error_handling": True,
                "logging": True,
                "monitoring": True,
                "graceful_degradation": True
            }
        }
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get system health status."""
        return {
            "status": "healthy" if self.metrics["evaluations_failed"] == 0 else "degraded",
            "evaluations_total": self.metrics["evaluations_total"],
            "success_rate": self.metrics["evaluations_successful"] / max(1, self.metrics["evaluations_total"]),
            "error_count": len(self.metrics["errors"]),
            "rate_limiter_status": "active",
            "input_validator_status": "active"
        }


async def run_generation_2_demo():
    """Run Generation 2 robust demonstration."""
    print("ğŸ”§ GENERATION 2 DEMO - MAKE IT ROBUST")
    print("=" * 60)
    print("Testing enhanced functionality with robust error handling")
    print("=" * 60)
    
    # Initialize robust pipeline
    pipeline = RobustEvaluationPipeline()
    
    # Run robust evaluation
    report = await pipeline.run_robust_evaluation()
    
    # Display results
    print(f"\nğŸ“Š ROBUST EVALUATION RESULTS")
    print("=" * 60)
    
    if report["success"]:
        summary = report["evaluation_summary"]
        robust = report["robustness_metrics"]
        
        print(f"âœ… Evaluation Status: {'SUCCESS' if report['success'] else 'FAILED'}")
        print(f"âœ… Execution Time: {report['execution_time']:.2f}s")
        print(f"âœ… Scenarios Attempted: {summary['scenarios_attempted']}")
        print(f"âœ… Success Rate: {summary['success_rate']:.1%}")
        print(f"âœ… Pass Rate: {summary['pass_rate']:.1%}")
        print(f"âœ… Average Score: {summary['average_score']:.3f}")
        
        print(f"\nğŸ›¡ï¸ ROBUSTNESS FEATURES:")
        for feature, enabled in robust.items():
            status = "âœ…" if enabled else "âŒ"
            print(f"  {status} {feature.replace('_', ' ').title()}")
        
        print(f"\nğŸ“ˆ DETAILED METRICS:")
        metrics = report["detailed_metrics"]
        for metric, value in metrics.items():
            print(f"  â€¢ {metric.replace('_', ' ').title()}: {value:.3f}")
        
        if report.get("errors"):
            print(f"\nâš ï¸ ERRORS ({len(report['errors'])}):")
            for error in report["errors"][:5]:  # Show first 5 errors
                print(f"  â€¢ {error}")
    
    else:
        print(f"âŒ Evaluation Status: FAILED")
        print(f"âŒ Error: {report.get('error', 'Unknown error')}")
        print(f"âŒ Execution Time: {report['execution_time']:.2f}s")
    
    # Save results
    with open("generation_2_results.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"\nğŸ’¾ Results saved to generation_2_results.json")
    
    # Health check
    health = pipeline.get_health_status()
    print(f"\nğŸ©º SYSTEM HEALTH: {health['status'].upper()}")
    print(f"  Success Rate: {health['success_rate']:.1%}")
    print(f"  Error Count: {health['error_count']}")
    
    # Verify Generation 2 success criteria
    print(f"\nğŸ¯ GENERATION 2 SUCCESS CRITERIA:")
    
    criteria = {
        "error_handling": report["success"] or "error" in report,
        "input_validation": report.get("robustness_metrics", {}).get("input_validation", False),
        "rate_limiting": report.get("robustness_metrics", {}).get("rate_limiting", False),
        "logging_enabled": report.get("robustness_metrics", {}).get("logging_enabled", False),
        "monitoring_enabled": report.get("robustness_metrics", {}).get("monitoring_enabled", False),
        "security_checks": report.get("robustness_metrics", {}).get("security_checks", False),
        "graceful_degradation": True  # Demonstrated by completing even with errors
    }
    
    all_passed = all(criteria.values())
    
    for criterion, passed in criteria.items():
        status = "âœ…" if passed else "âŒ"
        print(f"  {status} {criterion.replace('_', ' ').title()}")
    
    print(f"\nğŸ† GENERATION 2 RESULT: {'âœ… PASSED' if all_passed else 'âŒ FAILED'}")
    
    if all_passed:
        print("ğŸ‰ Generation 2 (MAKE IT ROBUST) completed successfully!")
        print("ğŸš€ Ready to proceed to Generation 3 (MAKE IT SCALE)")
    else:
        print("âš ï¸  Generation 2 has issues that need to be resolved")
    
    return {
        "success": all_passed,
        "report": report,
        "criteria": criteria
    }


if __name__ == "__main__":
    # Run the demonstration
    result = asyncio.run(run_generation_2_demo())
    
    # Exit with appropriate code
    exit(0 if result["success"] else 1)